{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72_7fqHwHFS5"
      },
      "outputs": [],
      "source": [
        "# With all variables\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import learning_curve\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# define the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# define a range of values for the regularization parameter C to test\n",
        "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# use grid search to find the best value of C\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='f1')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# print the best value of C and the corresponding F1 score on the validation set\n",
        "print(\"Best parameter:\", grid_search.best_params_)\n",
        "print(\"Validation set F1 score:\", grid_search.best_score_)\n",
        "\n",
        "# evaluate the performance of the model on the test set using the best parameter found\n",
        "model_best = LogisticRegression(C=grid_search.best_params_['C'])\n",
        "model_best.fit(X_train, y_train)\n",
        "y_pred_lr = model_best.predict(X_val)\n",
        "\n",
        "# get the classification report and confusion matrix\n",
        "print(classification_report(y_val, y_pred_lr))\n",
        "print(confusion_matrix(y_val, y_pred_lr))\n",
        "\n",
        "# Generate confusion matrix plot\n",
        "cm = confusion_matrix(y_val, y_pred_lr)\n",
        "sns.heatmap(cm, annot=True, cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n",
        "\n",
        "# calculate the AUC score and plot the ROC curve\n",
        "y_pred_proba = model_best.predict_proba(X_val)[:,1]\n",
        "auc = roc_auc_score(y_val, y_pred_proba)\n",
        "fpr, tpr, thresholds = roc_curve(y_val, y_pred_proba)\n",
        "plt.plot(fpr, tpr, label=f'AUC = {auc:.3f}')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend()\n",
        "\n",
        "# plot the learning curve\n",
        "train_sizes, train_scores, test_scores = learning_curve(\n",
        "    model_best, X_train, y_train, cv=5, n_jobs=-1,\n",
        "    train_sizes=np.linspace(0.1, 1.0, 10), scoring='f1'\n",
        ")\n",
        "train_scores_mean = np.mean(train_scores, axis=1)\n",
        "train_scores_std = np.std(train_scores, axis=1)\n",
        "test_scores_mean = np.mean(test_scores, axis=1)\n",
        "test_scores_std = np.std(test_scores, axis=1)\n",
        "plt.figure()\n",
        "plt.title('Learning Curve')\n",
        "plt.xlabel('Training Examples')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                 train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                 color='r')\n",
        "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                 test_scores_mean + test_scores_std, alpha=0.1, color='g')\n",
        "plt.plot(train_sizes, train_scores_mean, 'o-', color='r',\n",
        "         label='Training F1 Score')\n",
        "plt.plot(train_sizes, test_scores_mean, 'o-', color='g',\n",
        "         label='Cross-Validation F1 Score')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "##Precision recall plot\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# calculate precision and recall for different probability thresholds\n",
        "precision, recall, thresholds = precision_recall_curve(y_val, y_pred_proba)\n",
        "\n",
        "# calculate the average precision score\n",
        "average_precision = average_precision_score(y_val, y_pred_proba)\n",
        "\n",
        "# plot the precision-recall curve\n",
        "plt.plot(recall, precision, label=f'AP = {average_precision:.3f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ]
}