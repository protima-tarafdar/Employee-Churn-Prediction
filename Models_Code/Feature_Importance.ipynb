{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Categorical Feature Importance - Chi Square Test\n"
      ],
      "metadata": {
        "id": "9cxxR_QjBOg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Chi Square Test for Categorical features - feature Selection\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "cat_data = df.select_dtypes(include=['object'])\n",
        "# Separate your features (X) and target variable (y)\n",
        "X = cat_data\n",
        "\n",
        "# Convert your categorical features into numeric using one-hot encoding or label encoding\n",
        "X_encoded = pd.get_dummies(X)\n",
        "\n",
        "# Compute the chi-square statistic and p-values for each feature using chi2_contingency\n",
        "chi2, _, p_values, _ = chi2_contingency(X_encoded)\n",
        "\n",
        "# Create a DataFrame with the results\n",
        "results = pd.DataFrame({'feature': X_encoded.columns, 'chi2': chi2, 'p-value': p_values})\n",
        "\n",
        "# Sort the DataFrame by the p-values in ascending order\n",
        "results = results.sort_values('p-value')\n",
        "\n",
        "# Select the top k features with the highest chi-square statistic and p-values\n",
        "k = 30\n",
        "selected_features = results[:k]['feature']\n",
        "\n",
        "# Print the selected features\n",
        "print(selected_features)"
      ],
      "metadata": {
        "id": "0Lq9u6f3BT3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Categorical & Numerical Feature Importance - ANOVA Test"
      ],
      "metadata": {
        "id": "X8ZilPyTBYO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import f_regression, SelectKBest\n",
        "\n",
        "# Applying SelectKBest class to extract top 20 best features\n",
        "fs = SelectKBest(score_func=f_regression,k=15)\n",
        "# Applying feature selection\n",
        "fit = fs.fit(X_train,y_train)\n",
        "\n",
        "features_score = pd.DataFrame(fit.scores_)\n",
        "features = pd.DataFrame(X_train.columns)\n",
        "feature_score = pd.concat([features,features_score],axis=1)\n",
        "# Assigning column names\n",
        "feature_score.columns = [\"Input_Features\",\"F_Score\"]\n",
        "print(feature_score.nlargest(15,columns=\"F_Score\"))\n"
      ],
      "metadata": {
        "id": "lwzvSwasBgeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature Importance using XGBoost"
      ],
      "metadata": {
        "id": "ZvlkZTIyCdJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#xgb with all features on validation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "\n",
        "xgb_clf = xgb.XGBClassifier()\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "importances = xgb_clf.feature_importances_\n",
        "\n",
        "num_features = X_train.select_dtypes(include=np.number).columns.tolist()\n",
        "cat_features = X_train.columns.tolist()[len(num_features):]\n",
        "\n",
        "feature_importances = dict()\n",
        "for i, col in enumerate(num_features + cat_features):\n",
        "    if col in cat_features:\n",
        "        # if the feature is categorical, sum up the importances of its one-hot encoded columns\n",
        "        cat_col_importance = sum(importances[X_train.columns.str.startswith(col + '_')])\n",
        "        feature_importances[col] = cat_col_importance\n",
        "    else:\n",
        "        feature_importances[col] = importances[i]\n",
        "\n",
        "# Sort feature importances in descending order\n",
        "feature_importances = dict(sorted(feature_importances.items(), key=lambda x: x[1], reverse=True))\n",
        "\n",
        "# Print feature importances\n",
        "for col, importance in feature_importances.items():\n",
        "    print(f\"{col}: {importance}\")\n",
        "\n",
        "# Create a DataFrame with the feature importances\n",
        "df_importance = pd.DataFrame(feature_importances.items(), columns=['Feature', 'Importance'])\n",
        "\n",
        "# Display the DataFrame\n",
        "display(df_importance)\n",
        "\n",
        "# Create a horizontal bar chart\n",
        "plt.barh(range(len(feature_importances)), list(feature_importances.values()), align='center')\n",
        "plt.yticks(range(len(feature_importances)), list(feature_importances.keys()))\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Feature Importance')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "#y_pred = clf.predict(X_val)"
      ],
      "metadata": {
        "id": "OIny0yHyCf7a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}