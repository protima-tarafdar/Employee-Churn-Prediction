{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCKMuhKyF2Fa"
      },
      "outputs": [],
      "source": [
        "#Random Forest with best parameters\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {'n_estimators': [10, 50, 100],\n",
        "              'max_depth': [5, 10, 15],\n",
        "              'min_samples_split': [2, 5, 10]}\n",
        "\n",
        "# Create a random forest classifier\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid_search = GridSearchCV(estimator=rf_clf, param_grid=param_grid, cv=5)\n",
        "\n",
        "# Fit the grid search to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Evaluate the model with the best hyperparameters on the validation set\n",
        "y_pred_rf = grid_search.predict(X_val)\n",
        "accuracy = accuracy_score(y_val, y_pred_rf)\n",
        "print(\"Accuracy with best hyperparameters:\", accuracy)\n",
        "\n",
        "# Extract feature importances\n",
        "importances = grid_search.best_estimator_.feature_importances_\n",
        "features = X_train.columns\n",
        "\n",
        "# Sort feature importances in descending order\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "X_train\n",
        "\n",
        "# Rearrange feature names so they match the sorted feature importances\n",
        "names = [features[i] for i in indices]\n",
        "\n",
        "# Create plot\n",
        "plt.figure()\n",
        "\n",
        "# Create plot title\n",
        "plt.title(\"Feature Importance\")\n",
        "\n",
        "# Add bars\n",
        "plt.bar(range(X_train.shape[1]), importances[indices])\n",
        "\n",
        "# Add feature names as x-axis labels\n",
        "plt.xticks(range(X_train.shape[1]), names, rotation=90)\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, roc_curve, auc\n",
        "\n",
        "# Calculate accuracy and f1 score\n",
        "accuracy = accuracy_score(y_val, y_pred_rf)\n",
        "f1 = f1_score(y_val, y_pred_rf)\n",
        "\n",
        "print(classification_report(y_val, y_pred_rf))\n",
        "print(confusion_matrix(y_val, y_pred_rf))\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(y_val, y_pred_rf)\n",
        "\n",
        "# Calculate ROC curve and AUC\n",
        "y_pred_proba_rf = grid_search.predict_proba(X_val)[:, 1]\n",
        "fpr, tpr, thresholds = roc_curve(y_val, y_pred_proba_rf)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Print accuracy, f1 score, and confusion matrix\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"F1 score:\", f1)\n",
        "print(\"Confusion matrix:\\n\", cm)\n",
        "\n",
        "cm = confusion_matrix(y_val, y_pred_rf)\n",
        "sns.heatmap(cm, annot=True, cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label='AUC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    }
  ]
}